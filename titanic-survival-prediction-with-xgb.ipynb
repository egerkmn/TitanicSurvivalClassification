{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Welcome on board! \n\nThis is my first notebook upload, or lets say challenge, to Kaggle Competitions. Hope you enjoy!\n\nBefore we start, I believe that this notebook will help you to see how a classification problem can be handled from scratch. We will cover some common steps and try to make heuristic comments.\n\nIf you feel overwhelmed with python implementations, you can check Python Data Science Handbook by Jake VanderPlas.\nHere: https://jakevdp.github.io/PythonDataScienceHandbook/\n\nLet's dive into the challenge. We have a **binary classification problem** for Titanic Survivals and our evaluation metric is **accuracy** as mentioned in the problem summary. Survival percentage is given as **%38** in the training dataset which means we have an imbalanced dataset. My objective is to use four different ML algorithms which are **K-Nearest Neighbor(KNN), Random Forest(RF), Light Gradient Boosting Machine(LGBM), and Extreme Gradient Boosting(XGB)** as baseline models. After selecting the top performer(XGB), Hyperparameter search will be implemented to obtain final predictions for the submission.\n\nWe will go through some common steps:<br>\n1) [Preparing libraries and data](#1)<br>\n2) [Exploratory data analysis(EDA)](#2)<br>\n3) [Preprocess the data](#3)<br>\n4) [Model selection and hyperparameter search](#4)<br>\n5) [Final prediction and export](#5)","metadata":{}},{"cell_type":"markdown","source":"# 1) Preparing Libraries and Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preprocess libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer # to deal with missing values\nfrom sklearn.preprocessing import MinMaxScaler # for scaling\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold # for hyperparameter search\n\n# Models\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nplt.style.use('ggplot') \ncolor_pal = sns.color_palette()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:00.135525Z","iopub.execute_input":"2023-01-29T12:00:00.136014Z","iopub.status.idle":"2023-01-29T12:00:00.145779Z","shell.execute_reply.started":"2023-01-29T12:00:00.135974Z","shell.execute_reply":"2023-01-29T12:00:00.144781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain Data\ndf = pd.read_csv(\"../input/titanic/train.csv\")\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:00.666013Z","iopub.execute_input":"2023-01-29T12:00:00.666599Z","iopub.status.idle":"2023-01-29T12:00:00.695459Z","shell.execute_reply.started":"2023-01-29T12:00:00.666566Z","shell.execute_reply":"2023-01-29T12:00:00.694617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notes on investigating the data \nBefore EDA part, we should learn the variables and derive some facts from the data. First, let's give an brief introduction of the data by investigating the shape, numerical&categorical variables, missing values, and distribution of the target. Our aim is **to reduce the number of variables without losing information about the target** by looking at their missing percentage and effectiveness on the target (for example \"PassengerId\" variable may not be related to the target.) \n\nLets review:\n * Describing variables\n * Numerical and categorical variables\n * Missing values\n * Distribution of the target variable","metadata":{}},{"cell_type":"code","source":"# Brief Introduction of Data\nprint(\"---------------Describing Numerical Variables---------------\")\ndisplay(df.describe())\nprint(\"---------------Describing Categorical Variables---------------\")\ndisplay(df.describe(include=\"object\"))\nprint(\"---------------Feature Information---------------\")\ndisplay(df.info())\nprint(\"---------------Missing Value Percentage---------------\")\nprint(round(df.isnull().sum()/df.shape[0]*100,1))","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:01.765607Z","iopub.execute_input":"2023-01-29T12:00:01.766277Z","iopub.status.idle":"2023-01-29T12:00:01.843315Z","shell.execute_reply.started":"2023-01-29T12:00:01.766238Z","shell.execute_reply":"2023-01-29T12:00:01.841986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pie chart for the survival rate \nplt.pie(x=df[\"Survived\"].value_counts(), labels=df[\"Survived\"].value_counts() \\\n        .index.map({1:\"Survived\",0:\"Not Survived\"}), autopct='%.0f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:02.164688Z","iopub.execute_input":"2023-01-29T12:00:02.165934Z","iopub.status.idle":"2023-01-29T12:00:02.266496Z","shell.execute_reply.started":"2023-01-29T12:00:02.165856Z","shell.execute_reply":"2023-01-29T12:00:02.264913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After a quick investigation on the data, some processes should be performed:\n * Some variables irrelevant to the target should be removed. In that manner, PassengerID, Name and Ticket are not related to both the target and binary classification problem. \n * Cabin variable has 77% missing values which can not be appropriately filled.\n * We have some variables which can be treated as ordinal and binary variables instead of categorical variables. SibSp, Parch, and Pclass can be treated as ordinal(numerical). Sex can be treated as a binary variable since it has only 2 classes.\n \nPassengerId, Name, Cabin, and Ticket variables are removed.","metadata":{}},{"cell_type":"code","source":"# Preprocess part 1\ndef initial_preprocess(df):\n    df.drop([\"PassengerId\",\"Name\",\"Cabin\",\"Ticket\"], axis=1, inplace=True) #remove passenger Id, NAME, cabin, ticket\n    df[\"Sex\"] = df[\"Sex\"].replace({\"female\":0, \"male\":1}).astype(\"int\")\n    return df\ndf = initial_preprocess(df)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:02.993585Z","iopub.execute_input":"2023-01-29T12:00:02.994054Z","iopub.status.idle":"2023-01-29T12:00:03.004139Z","shell.execute_reply.started":"2023-01-29T12:00:02.994018Z","shell.execute_reply":"2023-01-29T12:00:03.002922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Exploratory Data Analysis (EDA)\n\nIn this step, we will discover some trends and facts by visualizing the variables. We will try to answer some questions like which gender is more likely to survive, whether we have outliers in Age or which variables are more related to the sale prices(target).","metadata":{}},{"cell_type":"code","source":"# Lets group the variables for visualization purposes.\nlist_categorical = [\"Embarked\"]\nlist_numerical = [\"Age\", \"Fare\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\n# For visualization purposes, I also added Sex and Pclass variables into categorical list\nlist_categorical_viz = [\"Embarked\",\"Sex\",\"Pclass\"]","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:03.841502Z","iopub.execute_input":"2023-01-29T12:00:03.842027Z","iopub.status.idle":"2023-01-29T12:00:03.847673Z","shell.execute_reply.started":"2023-01-29T12:00:03.841989Z","shell.execute_reply":"2023-01-29T12:00:03.846528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization for Categorical Variables\n\nWe will check the distribution and survival rates for each categorical variable.","metadata":{}},{"cell_type":"code","source":"# Bar plots for categorical variables (relative frequency) vs survival rates\n\ndef plot_survive_rates(df,var,hue):\n    xtick_order = df[var].value_counts().index.sort_values()\n\n    g = sns.barplot(x=var, y=hue, data=df, saturation=0.5,ci=None, order=xtick_order)\n    g.set(ylim=(0, 1))\n    \n    plt.xlabel(var,fontsize=16)\n    plt.ylabel(hue, fontsize=16)\n    \n    # Remove the graph borders\n    for spine in g.spines:\n        g.spines[spine].set_visible(False)\n  \n    # Add rotation to x labels      \n    plt.xticks(rotation = 45)\n\n    # Get current axis on current figure\n    ax = plt.gca()\n    # Iterate through the list of axes' patches (Patches are Artists with a face color and an edge color.)\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%0.2f' % p.get_height(), \n              fontsize=12, color='darkred', ha='center', va='bottom');\n","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:04.696657Z","iopub.execute_input":"2023-01-29T12:00:04.697098Z","iopub.status.idle":"2023-01-29T12:00:04.707283Z","shell.execute_reply.started":"2023-01-29T12:00:04.697061Z","shell.execute_reply":"2023-01-29T12:00:04.705826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot for each feature\nplt.figure(figsize = [10, int(np.ceil(len(list_categorical_viz)/3))*5]) \n\ni=0\nfor i in range(len(list_categorical_viz)): \n    plt.subplot(int(np.ceil(len(list_categorical_viz)/3)),3,i+1)\n    plot_survive_rates(df,list_categorical_viz[i],\"Survived\")\n\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:05.140032Z","iopub.execute_input":"2023-01-29T12:00:05.140473Z","iopub.status.idle":"2023-01-29T12:00:05.497038Z","shell.execute_reply.started":"2023-01-29T12:00:05.140433Z","shell.execute_reply":"2023-01-29T12:00:05.496120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n* First class passengers have higher survival rates comparing to other classes.\n* Female passengers have higher survival rates.\n* Passengers from Cherbourg have higher survival rates. \n\nWe have some answers for survival rates. However, we should also see variable pair relations to get more information (bivariate analysis). ","metadata":{}},{"cell_type":"markdown","source":"### Visualization for Numerical Variables\n\nLets perform some analysis to investigate the numerical variables\n * Univariate Analysis by using Boxplots and Histograms\n * Bivariate Analysis by using Pairplot\n * Multivariate Analysis by using Heatmap","metadata":{}},{"cell_type":"code","source":"def plot_hist(df,var):\n    bw=(df[var].max()-df[var].min())/20\n    g = sns.histplot(x = var, data = df, binwidth=bw)\n    g.set(xlabel=\"\",ylabel=\"\")\n    g.set_title('{}'.format(var))\n    \ndef histViz(df,num_list):\n    ncol=4\n    plt.figure(figsize = [20, int(np.ceil(len(num_list)/ncol))*5]) \n    plt.tight_layout()    \n\n    for i in range(len(num_list)):\n        plt.subplot(int(np.ceil(len(num_list)/ncol)),ncol,i+1)\n        plot_hist(df,num_list[i])\n\n    plt.show()\n    \nhistViz(df,list_numerical)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:06.543097Z","iopub.execute_input":"2023-01-29T12:00:06.543804Z","iopub.status.idle":"2023-01-29T12:00:07.963775Z","shell.execute_reply.started":"2023-01-29T12:00:06.543768Z","shell.execute_reply":"2023-01-29T12:00:07.962529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_box(df,var):\n    bw=(df[var].max()-df[var].min())/20\n    g = sns.boxplot(y = var, data = df, medianprops={'color':'c'})\n    g.set(xlabel=\"\",ylabel=\"\")\n    g.set_title('{}'.format(var))\n    \ndef BoxViz(df,num_list):\n    ncol=4\n    plt.figure(figsize = [20, int(np.ceil(len(num_list)/ncol))*5]) \n    plt.tight_layout()    \n\n    for i in range(len(num_list)):\n        plt.subplot(int(np.ceil(len(num_list)/ncol)),ncol,i+1)\n        plot_box(df,num_list[i])\n\n    plt.show()\n    \nBoxViz(df,list_numerical)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:07.965611Z","iopub.execute_input":"2023-01-29T12:00:07.966034Z","iopub.status.idle":"2023-01-29T12:00:08.660638Z","shell.execute_reply.started":"2023-01-29T12:00:07.966000Z","shell.execute_reply":"2023-01-29T12:00:08.659287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** We can obtain some facts for the numerical variables by comparing their distributions. Some examples are given below.\n\n * Our variables do not seem to have normal distribution. (May be Age)\n * We have outliers in some variables like Fare. We can also remove outliers but it is not implemented in this notebook.","metadata":{}},{"cell_type":"markdown","source":"### Bivariate Analysis - EDA","metadata":{}},{"cell_type":"code","source":"#pairplot\nsns.pairplot(df, hue=\"Survived\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:08.856340Z","iopub.execute_input":"2023-01-29T12:00:08.856765Z","iopub.status.idle":"2023-01-29T12:00:19.490483Z","shell.execute_reply.started":"2023-01-29T12:00:08.856729Z","shell.execute_reply":"2023-01-29T12:00:19.489374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n* Pclass vs. Fare: the first class passengers have higher fares and they seem more likely to survive.\n* Age Distribution (age vs. age): it seems younger passengers have higher survival rates.  \n\nLets see the correlation map for our data.","metadata":{}},{"cell_type":"markdown","source":"### Multivariate Analysis - EDA","metadata":{}},{"cell_type":"code","source":"# Correlation map for numerical variables. We have some correlated variables. \nplt.figure(figsize=(8,8))\nsns.heatmap(df.corr(), cmap=\"viridis\")\nplt.title(\"Heatmap of Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:19.493097Z","iopub.execute_input":"2023-01-29T12:00:19.493592Z","iopub.status.idle":"2023-01-29T12:00:19.801807Z","shell.execute_reply.started":"2023-01-29T12:00:19.493543Z","shell.execute_reply":"2023-01-29T12:00:19.800361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlations for survival\ndf.corr()[\"Survived\"].to_frame().T.round(4)*100\n#It seems Fare and Pclass are correlated which makes sense!","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:19.803562Z","iopub.execute_input":"2023-01-29T12:00:19.804034Z","iopub.status.idle":"2023-01-29T12:00:19.825122Z","shell.execute_reply.started":"2023-01-29T12:00:19.803994Z","shell.execute_reply":"2023-01-29T12:00:19.823968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** Collinearity is an another issue for Machine Learning Algorithms, especially for parametric ones. For example, Pclass is related to the Fare. We could handle the collinearity by using dimensionality reduction methods such as PCA. We will let the model handle. \n\nWe have also listed the most related variables to the target above and they make sense. For example, Pclass is a key factor for the survival rate. ","metadata":{}},{"cell_type":"markdown","source":"# 3) PreProcessing\n\nIn this section, we will prepare the data for ML algorithms by handling missing values and performing one hot encoding for categorical variables and min-max scaling for numerical variables. \n\n* **Missing handling:** The most frequent class(mode) for categorical variables and median value for numerical variables will be used.<br>\n* **One Hot Encoding:** Since categorical variables can not be directly used in many models, we will use one hot encoding.<br>\n* **Scaling:** To reduce the effect of scale of some numerical variables like fare, MinMaxScaler will be used.\n\nWe will use ColumnTransformer and Pipeline for the processes, but first let's create our validation datasets.\n\n**Important Note:** Since the target is imbalanced, we should use stratification in both train_test split part and cross-validation(CV) in hyperparameter search part.","metadata":{}},{"cell_type":"code","source":"# Train Test Datasets\nX = df.drop(\"Survived\", axis=1)\ny = df[\"Survived\"]\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.20, stratify = y, random_state=1234)\nprint(f'Xtrain shape: {Xtrain.shape}, ytrain shape: {ytrain.shape}')\nprint(f'Xval shape: {Xval.shape}, yval shape: {yval.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:19.827773Z","iopub.execute_input":"2023-01-29T12:00:19.828233Z","iopub.status.idle":"2023-01-29T12:00:19.840839Z","shell.execute_reply.started":"2023-01-29T12:00:19.828190Z","shell.execute_reply":"2023-01-29T12:00:19.839422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_var = Xtrain.select_dtypes(include=\"object\").columns.to_list()\ncat_index_list = Xtrain.columns.to_frame(index=False,name=\"variables\").query('variables in @categorical_var').index.to_list()\n\n# Transformers for categorical values\ncat_pipe = Pipeline([('imputer_frequent', SimpleImputer(strategy=\"most_frequent\")),\n                     ('ohe', OneHotEncoder(handle_unknown=\"ignore\", sparse=False))])\n\n# Transformers for numerical values\nnum_pipe = Pipeline([('imputer_median', SimpleImputer(strategy=\"median\")), \n                     ('MinMax_scaling', MinMaxScaler())\n                     ])\n\nct = ColumnTransformer([('cat_pipe',cat_pipe,cat_index_list)], \n                       remainder=num_pipe)#num_pipe\n\nXtrain = pd.DataFrame(ct.fit_transform(Xtrain)) \nXval = pd.DataFrame(ct.transform(Xval))","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:19.842324Z","iopub.execute_input":"2023-01-29T12:00:19.842755Z","iopub.status.idle":"2023-01-29T12:00:19.873088Z","shell.execute_reply.started":"2023-01-29T12:00:19.842724Z","shell.execute_reply":"2023-01-29T12:00:19.872101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Xtrain shape: {Xtrain.shape}, ytrain shape: {ytrain.shape}')\nprint(f'Xval shape: {Xval.shape}, yval shape: {yval.shape} \\n')\nprint(\"Since OneHotEnconder is used, the number of variables is increased.\")","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:00:19.874566Z","iopub.execute_input":"2023-01-29T12:00:19.874934Z","iopub.status.idle":"2023-01-29T12:00:19.880780Z","shell.execute_reply.started":"2023-01-29T12:00:19.874880Z","shell.execute_reply":"2023-01-29T12:00:19.879431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Model Selection and Hyperparameter Search\n\nAfter the Preprocessing Section, our dataset is ready for modeling! Let's calculate the baseline scores for our models.","metadata":{}},{"cell_type":"code","source":"# For baseline accuracies\ndef evaluationMetrics(clf,Xtrain,Xval,ytrain,yval,refit=True):\n    \n    dict_output= {}\n    \n    if refit:\n        clf.fit(Xtrain,ytrain)\n    ytrainPred=clf.predict(Xtrain)\n    ytestPred=clf.predict(Xval)\n    \n    #train lists\n    # we can also add f1 score or ROC AUC as metrics\n    dict_output[\"clf_name\"]=type(clf).__name__\n    dict_output[\"train_acc\"]=accuracy_score(ytrain,ytrainPred)\n    #test lists\n    dict_output[\"val_acc\"]=accuracy_score(yval,ytestPred)\n\n    return dict_output","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:01:01.855384Z","iopub.execute_input":"2023-01-29T12:01:01.855802Z","iopub.status.idle":"2023-01-29T12:01:01.863223Z","shell.execute_reply.started":"2023-01-29T12:01:01.855767Z","shell.execute_reply":"2023-01-29T12:01:01.861846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifiers\nclf_list=[KNeighborsClassifier(n_neighbors=5),\n          RandomForestClassifier(n_estimators = 200),\n          LGBMClassifier(objective=\"binary\"),# since we have a binary classification problem\n          XGBClassifier(objective=\"binary:logistic\") # since we have a binary classification problem\n          ]\n\nfor i in range(len(clf_list)):\n    if i == 0:\n        df_eval=pd.DataFrame(evaluationMetrics(clf_list[i],Xtrain,Xval,ytrain,yval),index=[0])\n    else:\n        df_eval=df_eval.append(evaluationMetrics(clf_list[i],Xtrain,Xval,ytrain,yval), ignore_index=True)\n\ndisplay(df_eval)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharex=True)\ng = sns.barplot(data=df_eval, x='clf_name', y='train_acc', ax=ax1)\nfor p in g.patches:\n  percentage = '{:.0f}%'.format(100*p.get_height())\n  x = p.get_x() + p.get_width()/2\n  y = p.get_height() *0.5\n  g.annotate(percentage, (x, y),ha='center', size='medium', color='darkred', weight='semibold')\n\ng = sns.barplot(data=df_eval, x='clf_name', y='val_acc', ax=ax2)\nfor p in g.patches:\n  percentage = '{:.0f}%'.format(100*p.get_height())\n  x = p.get_x() + p.get_width()/2\n  y = p.get_height() *0.5\n  g.annotate(percentage, (x, y),ha='center', size='medium', color='darkred', weight='semibold')\n\nax1.set_xticks(ax1.get_xticks(), ax1.get_xticklabels(), rotation=90)\nax2.set_xticks(ax2.get_xticks(), ax2.get_xticklabels(), rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:01:02.433589Z","iopub.execute_input":"2023-01-29T12:01:02.434054Z","iopub.status.idle":"2023-01-29T12:01:03.839718Z","shell.execute_reply.started":"2023-01-29T12:01:02.434015Z","shell.execute_reply":"2023-01-29T12:01:03.838397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As baseline scores, **XGB** outperformed the others for **both training and test results**. Let's try to search hyperparameters for XGB (the results may differ in each run).\n\nBefore Hyperparameter search, I would like to mention some methods to increase model performances for imbalanced datasets. \n * The first one is resampling methods which you can generate synthetic minorty class to balance the data (as oversampling) or select some data from majority class to balance the data (as undersampling). There are many approaches for both oversampling and undersampling (for further: please check imblearn library). \n * The second one is class weight. Most models have their own class weight parameter to deal with imbalanced datasets. You can use sklearn.utils to calculate class weights or you can calculate by your own. In this notebook, I leave the following cell for class weight calculation, but I will not use in the model (I tried and got worse results).","metadata":{}},{"cell_type":"code","source":"# for illustration of the class weight calculation.\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(ytrain), y=ytrain)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T12:58:35.520814Z","iopub.execute_input":"2023-01-26T12:58:35.521348Z","iopub.status.idle":"2023-01-26T12:58:35.530044Z","shell.execute_reply.started":"2023-01-26T12:58:35.521299Z","shell.execute_reply":"2023-01-26T12:58:35.528442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the hyperparameter search of XGB, number of estimators, min child weight, max depth, and subsample are used with RandomizedSearchCV and the final result is shown below. Randomized Search is used instead of Grid Search due to the computation power limitations.\n\nFor further information about XGB: https://xgboost.readthedocs.io/en/stable/parameter.html","metadata":{}},{"cell_type":"code","source":"# Tuning XGBOOST \ncv = StratifiedKFold(n_splits = 5, shuffle = True)\n\nXGBParams = {\"n_estimators\" : [250,500,1000,2000],\n            \"learning_rate\": [0.01, 0.1, 0.3, 0.5, 1], #default=0.3   \n            \"min_child_weight\": range(1,12,2), #default=1  \n            \"max_depth\": range(6,13,2), #default=6\n            \"subsample\": [0.2, 0.5, 0.8, 1]} #default=1\n\nrscv_xgb = RandomizedSearchCV(XGBClassifier(objective=\"binary:logistic\"), XGBParams,\n                          cv = cv, return_train_score = True, n_iter=10) #you can increase for better results\n\nrscv_xgb.fit(Xtrain, ytrain)\nyhat2 = rscv_xgb.predict(Xval)\nyhat_train2 = rscv_xgb.predict(Xtrain)\n\nprint(\"Best n_estimators:\", rscv_xgb.best_estimator_.n_estimators)\nprint(\"Best learning_rate:\", rscv_xgb.best_estimator_.learning_rate)\nprint(\"Best min_child_weight:\", rscv_xgb.best_estimator_.min_child_weight)\nprint(\"Best max_depth:\", rscv_xgb.best_estimator_.max_depth)\nprint(\"Best subsample:\", rscv_xgb.best_estimator_.subsample)\nprint(f\"Train Accuracy: {round(accuracy_score(ytrain, yhat_train2),4)}, Test Accuracy: {round(accuracy_score(yval, yhat2),4)}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:15:24.313959Z","iopub.execute_input":"2023-01-29T12:15:24.314770Z","iopub.status.idle":"2023-01-29T12:18:13.777033Z","shell.execute_reply.started":"2023-01-29T12:15:24.314726Z","shell.execute_reply":"2023-01-29T12:18:13.776100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After Hyperparameter search, we increased the test accuracy to **83%** by **sacrificing training accuracy around 10%**. We could tune more parameter to increase test accuracy, but it is enough for this brief notebook.\n\nNote: Do not forget that it is a random search. The results can vary depending on random states!","metadata":{}},{"cell_type":"markdown","source":"# 5) Final Prediction and Export\n\nLet's preprocess the test dataset by using our preprocess function and ColumnTransformer, then submit it.","metadata":{}},{"cell_type":"code","source":"# preprocess for test \ndf_test = initial_preprocess(df_test)\ndf_test = pd.DataFrame(ct.transform(df_test))\n\n# Test prediction with our model.\nytest_hat = rscv_xgb.predict(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:18:21.943751Z","iopub.execute_input":"2023-01-29T12:18:21.944147Z","iopub.status.idle":"2023-01-29T12:18:21.981236Z","shell.execute_reply.started":"2023-01-29T12:18:21.944115Z","shell.execute_reply":"2023-01-29T12:18:21.980173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exporting the submission file\ndf_tmp = pd.read_csv(\"../input/titanic/test.csv\")\ndf_tmp2 = df_tmp[[\"PassengerId\"]]\ndf_final = df_tmp2.merge(pd.DataFrame(ytest_hat), left_index=True, right_index=True)\ndf_final.rename(columns={0:\"Survived\"}).set_index(\"PassengerId\").to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-29T12:18:23.169282Z","iopub.execute_input":"2023-01-29T12:18:23.169669Z","iopub.status.idle":"2023-01-29T12:18:23.192860Z","shell.execute_reply.started":"2023-01-29T12:18:23.169637Z","shell.execute_reply":"2023-01-29T12:18:23.191951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you found this notebook helpful, some upvotes would be very much appreciated! :) \n\nYou can also check my second notebook for a regression problem in Kaggle: https://www.kaggle.com/code/egeerkmen/catboost-and-xgb-with-detailed-eda","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}